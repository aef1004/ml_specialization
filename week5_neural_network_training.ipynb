{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deeb0732-cf53-4976-b2d9-df3d2ba4f412",
   "metadata": {},
   "source": [
    "# Week 5: Neural Network Training\n",
    "\n",
    "- Neural network training\n",
    "- Activation functions\n",
    "- Multiclass classification (MNIST)\n",
    "- Softmax regression model for multiclass classification\n",
    "- Adam Algorithm Intuition\n",
    "- Convolutional Neural Network\n",
    "\n",
    "\n",
    "General Training Steps\n",
    "1. specifcy how to compute output given inpux x and parameters w, b to define model as fw,b(x)\n",
    "2. specifcy loss and cost L, J(w,b)\n",
    "3. train on data to minimize J(w,b)\n",
    "\n",
    "### Step 1 for neural networks\n",
    "\n",
    "model = Sequential([\n",
    "Dense(units = ...)\n",
    "Dense(...)\n",
    "])\n",
    "\n",
    "### Step 2\n",
    "Loss function examples\n",
    " - MeanSquaredError() for example if we're predicting numbers and not categories\n",
    " - BinaryCross() should only be used for classification with exactly 2 cases\n",
    " - SparseCategoricalCrossentropy() used for Softmax\n",
    "\n",
    "Implementation\n",
    "model.complie(loss = BinaryCrossentroypy(from_logits = True))\n",
    "\n",
    "from_logits = True fixes some rounding errors\n",
    "\n",
    "### Step 3\n",
    "model.fit(X,y, epochs = 100)\n",
    "\n",
    "\n",
    "**Activation functions**\n",
    "- Sigmoid g(z) = 1/(1+e^-z)\n",
    "    - typically used for binary classification\n",
    "    - but could also be used in multilabel classification: is it a car? is it a bus? is it a pedestrian?\n",
    "- Linear activation function g(z) = z\n",
    "    - if linear activation (with positive or negative values)\n",
    "- ReLu (rectified linear unit) g(z) = max(0,z)\n",
    "    - if y can only have positive values or 0\n",
    "    - most common to use in activation layers\n",
    "- Softmax(multiclass classification)\n",
    "    - we can either set activation = softmax OR activation = linear and from_logit = True - this will be more accurate - see optional lab below\n",
    " \n",
    "*if all g(z) are linear, then it's no different from a linear regression\n",
    "\n",
    "\n",
    "**Multiclass Classification (MNIST)**\n",
    "\n",
    "example: trying to identify handwritten numbers 0-9\n",
    "Y can take on a set number of discrete numbers\n",
    "Can use Softmax\n",
    "\n",
    "Softmax\n",
    "z = w . x + b\n",
    "all possible z outputs (e^z1 / sum(all e^z)) should equal to 1\n",
    "if the final output layer has more than 1 output\n",
    "\n",
    "\n",
    "**Adam Algorithm Intuition (ADAptive Moment Intuition**\n",
    "\n",
    "- faster than gradient descent\n",
    "\n",
    "Add to model.compile\n",
    "model.comile(optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3, loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logit = True)\n",
    "\n",
    "\n",
    "**Convolutional Neural Network**\n",
    "\n",
    "example: looking at a window of values (like a window function of an EKG)\n",
    "\n",
    "Can set the first layer in the neural network as a window of multiple values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf31f8-14b7-45b3-b68a-d16377f9f96c",
   "metadata": {},
   "source": [
    "### Optional Lab - Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c020695-fea8-4417-9833-f968d8da718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.datasets import make_blobs\n",
    "%matplotlib widget\n",
    "from matplotlib.widgets import Slider\n",
    "from lab_utils_common import dlc\n",
    "from lab_utils_softmax import plt_softmax\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1244b1-e95f-45f9-89a0-0825a140b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(z):\n",
    "    ez = np.exp(z)              #element-wise exponenial\n",
    "    sm = ez/np.sum(ez)\n",
    "    return(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075d391-cd1a-4919-b498-76c0b90ff84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make  dataset for example\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06310b-beda-4c19-bccb-e34705f2e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "preferred_model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'linear')   #<-- Note use this instead of softmax when using from_logits = True\n",
    "    ]\n",
    ")\n",
    "preferred_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "preferred_model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed259f-ebde-4c2e-856d-0072e18c8011",
   "metadata": {},
   "source": [
    "Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6f17d-c802-4d4c-809e-7882ea8fa82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_preferred = preferred_model.predict(X_train)\n",
    "print(f\"two example output vectors:\\n {p_preferred[:2]}\")\n",
    "print(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd1af47-a1f4-4422-95a0-e6764b3ec776",
   "metadata": {},
   "source": [
    "The output predictions are not probabilities!\n",
    "If the desired output are probabilities, the output should be be processed by a [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202c5cb-461b-461b-8593-f195acbc932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_preferred = tf.nn.softmax(p_preferred).numpy()\n",
    "print(f\"two example output vectors:\\n {sm_preferred[:2]}\")\n",
    "print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f208c-25f1-4050-bce3-110ffd5d503c",
   "metadata": {},
   "source": [
    "To select the most likely category, the softmax is not required. One can find the index of the largest output using [np.argmax()](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f10008-4bd3-4fba-810c-f45e9939dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
